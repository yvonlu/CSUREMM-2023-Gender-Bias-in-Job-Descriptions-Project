# CSUREMM 2023 Gender Bias in Job Descriptions Project

## Refining gender bias algorithms using sentiment analysis: ***an investigation of gender bias levels across industries, career progressions, and top-ranked companies for gender equality***

**Authors:**

Yvon Lu (Columbia College, 2025) 

Jordan Shiff (Barnard College, 2025)

Sophia Tu (Barnard College, 2025)

**Abstract:** 

The hiring process plays a crucial role in shaping the workforce composition, and biased practices within this process can perpetuate gender disparities. Previous research has revealed that the inclusion of gendered language in job advertisements can have an impact on gender imbalances in the application pool by discouraging qualified female applicants from applying to positions. Research has also found that women find job descriptions with more masculine language less appealing, anticipate less sense of belongingness at the company, and perceive the workplace to have less women  (Gaucher et al., 2011). In our approach, we considered the usage of sentiment analysis in addition to word embeddings and frequency of masculine and feminine words in job descriptions to assess for measuring gender bias levels in job descriptions. Our findings indicate that there is a statistical significant amount of difference in bias amongst industries with some more positively female biased (Education) and some more negatively female biased (Finance and Insurance, Retail Trade, etc). We also found a significant statistical difference in job qualifications and job description bias, with a negative (or male) bias associated for jobs with higher degrees


**Research Questions:**

1. How can we build a model to detect gender bias in job descriptions with industry-specific weights?
item 

2. How can we use this model to analyze gender imbalances in the hiring process for different educational requirements?

3. How do gender bias levels in job descriptions differ between top-ranked companies for gender equality and other companies?
